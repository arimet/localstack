# LocalStack : D√©velopper et Tester vos Lambda Functions en Local

Dans un de nos  projet, on a un backend assez particulier : du full SQL avec Hasura en proxy devant. C'est super efficace pour les requ√™tes classiques, mais parfois on a besoin de faire des trucs plus complexes en Python.
Et c'est l√† qu'AWS Lambda entre en jeu.

Le probl√®me ? Un developpement directement sur AWS, √ßa peut etre long et couter cher. Si vous devez passer par une √©quipe devops pour chaque modif, √ßa devient vite un cauchemar. Et vous ne pouvez pas tester localement. **LocalStack a chang√© la donne** : on peut maintenant d√©velopper et tester nos Lambdas en local

Cet article vous montre comment on a mis cette stack en places et les probl√®mes qu'on a rencontr√©s (spoiler : les Lambda Layers sur LocalStack, c'est payant).

## Lambda, c'est quoi exactement ?

AWS Lambda, c'est du **serverless**: un service de calcul qui ex√©cute du code sans qu'il soit n√©cessaire de g√©rer des serveurs. Vous √©crivez du code, AWS l'ex√©cute quand il faut, et vous payez uniquement le temps d'ex√©cution.

Et derriere c'est grosso-modo juste une fonction

```python
def lambda_handler(event, context):
    # event contient les donn√©es d'entr√©e
    # context donne des infos sur l'ex√©cution
    return {
        'statusCode': 200,
        'body': json.dumps({'message': 'Hello from Lambda!'})
    }
```

Dans notre cas, on utilise les Lambdas pour :
- Communiquer avec des S3
- Envoyer des emails via des CRONs
- Faire des calculs qui prendraient trop de temps en SQL

On peut cr√©er des lambdas via l'interface, soit par ligne de commande, c'est gal√®re. Il faut d√©finir l'infrastructure, g√©rer les permissions, d√©ployer le code...

C'est l√† que **SAM** devient indispensable.

## SAM : Infrastructure pour les non devops

AWS SAM (Serverless Application Model), c'est un framework qui simplifie drastiquement la cr√©ation de Lambdas. Tout se fait via un fichier YAML.

Un projet SAM ressemble √† √ßa

```
mon-projet/
‚îú‚îÄ‚îÄ template.yaml          # D√©finit toute l'infrastructure
‚îú‚îÄ‚îÄ samconfig.toml         # Config de d√©ploiement
‚îú‚îÄ‚îÄ src/handlers/          # Votre code Python
‚îî‚îÄ‚îÄ layers/                # Code partag√© entre Lambdas
```

Le fichier **template.yaml** est le c≈ìur du projet. Voici un exemple minimaliste :

```yaml
AWSTemplateFormatVersion: '2010-09-09'
Transform: AWS::Serverless-2016-10-31

Resources:
  HelloWorldFunction:
    Type: AWS::Serverless::Function
    Properties:
      FunctionName: hello-world-function
      CodeUri: src/handlers/hello_world/
      Handler: app.lambda_handler
      Runtime: python3.12
      Events:
        HelloWorld:
          Type: Api
          Properties:
            Path: /hello
            Method: get
```

C'est tout ! SAM va cr√©er la Lambda Avec une simple commande

```bash
sam build   # Build le projet
sam deploy  # D√©ploie sur AWS
```

Bien sur, vous pouvez ajouter des S3, des API Gateway, des permissions IAM, tout est g√©r√© dans le YAML. Nous n'allons pas nous √©tendre l√†-dessus, les docs SAM sont tr√®s compl√®tes (meme si bon courage pour trouver facilement l'information).

--------------- TODO ---------------
Expliquez comment SAM sait o√π deployer. Pas trouv√© l'info facilement.
-----------------------------------------------

Bon, c'est cool, mais √ßa ne r√©pond toujours pas √† une probl√©matique. A chaque fois qu'on veut tester, il faut d√©ployer sur une vrai instante.  **LocalStack** r√©sout ce probl√®me.

## LocalStack : AWS sur votre machine

LocalStack is a cloud service emulator that runs in a single container on your laptop or in your CI environment.

- **Gratuit** : Pas de facture AWS qui explose. Mais des features premiums (nous reviendrons dessus plus tard)
- **Rapide** : Deploy en 2 secondes au lieu de 2 minutes
- **Safe** : Vous cassez rien sur le vrai AWS

Pour nous, √ßa am√©liore notre fa√ßon de d√©velopper. On peut tester directement l'appel √† la lamda et non plus le code contenu dans la lamda (ca √† sa nuance !).

### Installation avec Docker

Un simple `docker-compose.yml` suffit

```yaml
version: '3.8'

services:
  localstack:
    image: localstack/localstack:latest
    ports:
      - "4566:4566"  # Port principal LocalStack
    environment:
      - SERVICES=lambda,apigateway,s3,cloudformation,logs
      - DEBUG=1
      - LAMBDA_EXECUTOR=docker
      - AWS_DEFAULT_REGION=eu-west-1
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock"
```

D√©marrez avec

#### D√©marrage

```bash
docker compose up -d
```

LocalStack tourne maintenant sur `http://localhost:4566`. Pour d√©ployer dessus au lieu d'AWS, on utilise les lignes de commande `awslocal` et `samlocal` qui sont en r√©alit√© des wrappers des commandes `aws` et `sam` mais qui pointent directement vers localstack.

### Installer les outils

```bash
pip install aws-sam-cli awscli-local
```

Ensuite, le workflow de dev devient trivial
```bash
# 1. Coder votre Lambda dans src/handlers/ et ajouter dans template.yaml

# 2. Build
samlocal build

# 3. Deploy sur LocalStack
samlocal deploy

# 4. Tester
awslocal lambda invoke \
    --function-name hello-world-function \
    response.json
    
cat response.json  # Voir le r√©sultat
```

Des modifs √† faire ? Modifier votre code, refaites `samlocal build && samlocal deploy`, c'est red√©ploy√© en quelques secondes. Et vous pouvez it√©rer rapidement

## Lambda Layers : Partager du code entre Lambdas

Au bout d'un moment, vous allez avoir plusieurs Lambdas. Et il se peut que ces lamdas partagent du code - des fonctions utilitaires, du formatage de r√©ponses etc...

Le soucis, c'est que les lambdas sont id√©pendante. Si vous avez une besoin d'une fonction entre deux lambdas, vous devez la copier-coller dans chaque Lambda.

Enfin nous mentons, car **les Lambda Layers r√®glent ce probl√®me.** Un Layer, c'est un package de code r√©utilisable que plusieurs Lambdas peuvent partager. 

Dans notre cas, on va cr√©er un Layer avec nos fonctions de formatage de r√©ponses, comme √ßa toutes nos Lambdas retournent le m√™me format JSON.

### Structure d'un Layer

```
layers/
‚îî‚îÄ‚îÄ custom_utils/
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ display.py          # Vos fonctions
    ‚îî‚îÄ‚îÄ requirements.txt    # D√©pendances (optionnel)
```

**display.py** :
```python
import json

def format_response(status_code, data):
    """Formatte une r√©ponse standard API Gateway"""
    return {
        "statusCode": status_code,
        "headers": {"Content-Type": "application/json"},
        "body": json.dumps(data)
    }

def get_greeting(name):
    """Message de salutation personnalis√©"""
    return f"Hello, {name}!"
```

Dans `template.yaml`, d√©clarez le Layer

```yaml
Resources:
  CustomUtilsLayer:
    Type: AWS::Serverless::LayerVersion
    Properties:
      LayerName: custom-utils-layer
      ContentUri: layers/custom_utils/
      CompatibleRuntimes:
        - python3.12

  HelloWorldFunction:
    Type: AWS::Serverless::Function
    Properties:
      CodeUri: src/handlers/hello_world/
      Handler: app.lambda_handler
      Layers:
        - !Ref CustomUtilsLayer  # Utilise le Layer
```

Dans votre Lambda, importez directement

```python
from custom_utils import format_response, get_greeting

def lambda_handler(event, context):
    name = event.get("queryStringParameters", {}).get("name", "World")
    greeting = get_greeting(name)
    return format_response(200, {"message": greeting})
```

Simple, √©l√©gant, r√©utilisable. Sauf que... **√ßa marche pas sur LocalStack** üòÖ

## Le Probl√®me des Layers sur LocalStack

Sur AWS, quand vous utilisez un Layer, AWS le monte automatiquement dans `/opt/python` (si vous developez en python bien sur) et tout fonctionne. Sur LocalStack... non. Vous d√©ployez, vous testez, et boom :

```
ModuleNotFoundError: No module named 'custom_utils'
```

LocalStack cr√©e bien le Layer, il l'associe √† votre Lambda, mais il ne le monte pas dans le container d'ex√©cution. Pourquoi ? Car c'est une feature premium de LocalStack Pro.

**La solution ?** Un petit workaround Docker. Pas tr√®s √©l√©gant, mais √ßa fonctionne !

### Le Workaround : Monter les Layers manuellement

L'id√©e : dire √† LocalStack de monter votre dossier `layers/` directement dans les containers Lambda via Docker volumes.

Modifiez votre `docker-compose.yml`

```yaml
services:
  localstack:
    image: localstack/localstack:latest
    ports:
      - "4566:4566"
    environment:
      - SERVICES=lambda,apigateway,s3,cloudformation,logs
      - DEBUG=1
      - LAMBDA_EXECUTOR=docker
      - LAMBDA_DOCKER_NETWORK=localstack-sam-network
      
      # üîß LE WORKAROUND : Monte les layers dans les containers Lambda
      - LAMBDA_DOCKER_FLAGS=-v /var/www/hackday/localstack-test/layers:/opt/python:ro
      
      - AWS_DEFAULT_REGION=eu-west-1
    volumes:
      - "/var/run/docker.sock:/var/run/docker.sock"
      
      # üîß LE WORKAROUND : Monte les layers dans LocalStack
      - "/var/www/hackday/localstack-test/layers:/opt/python:ro"
    networks:
      - localstack-sam-network

networks:
  localstack-sam-network:
    driver: bridge
```

**Important** : Remplacez `/var/www/hackday/localstack-test/layers` par le **chemin absolu** vers votre dossier layers.

### Comment √ßa marche ?

1. `LAMBDA_DOCKER_FLAGS` dit √† LocalStack : "Quand tu cr√©es un container Lambda, monte ce volume dedans"
2. Le volume monte votre dossier `layers/` dans `/opt/python` du container Lambda
3. Python peut maintenant importer depuis `/opt/python/custom_utils/`

### Les limitations du workaround

Soyons honn√™tes, ce workaround a des d√©fauts :

1. **Pas de versioning** : Tous les Lambdas utilisent la m√™me version du Layer
2. **Pas ISO a AWS** : Sur AWS, la structure est diff√©rente. Apr√®s vous pouvez toujours payer.
3. **Pas de hot-reload** : Meme si vous avez un volume mont√©, LocalStack "cr√©e" le package au moment du d√©ploiement. Donc si vous modifiez le code du Layer, il faut redeployer la Lambda. La version premium de LocalStack permet peut-√™tre le hot-reload, nous n'avons pas test√©.

## Conclusion

Voil√† comment on a setup notre environnement de dev Lambda. C'est pas parfait - le workaround des Layers est un hack - mais √ßa marche et √ßa nous fait gagner un temps fou.

**Le setup complet** :
1. LocalStack pour √©muler AWS en local
2. SAM pour d√©finir l'infrastructure
3. Un workaround Docker pour les Layers
4. Des Makefiles pour automatiser

**Ce qui change dans le quotidien** :
- Developement de bout en bout sur une feature en local
- Tests illimit√©s sans voir la facture AWS exploser
- Tout le monde a le m√™me environnement (docker-compose)

Pour notre use case (backend SQL + Hasura + Lambdas Python pour les traitements complexes), c'est le setup id√©al. On garde Hasura pour les requ√™tes CRUD classiques, et on sort l'artillerie Lambda quand on a besoin de Python.

Si vous √™tes dans une situation similaire, nous vous encourageons vraiment √† tester LocalStack. Oui, il y a des petits hacks √† faire (les Layers...), mais le gain en productivit√© est √©norme.

Et mention sp√©ciale √† SAM qui rend la gestion de l'infrastructure super simple.